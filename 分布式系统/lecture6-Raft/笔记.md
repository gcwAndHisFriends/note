# raft

如何正确获取replication中状态

问题:通过单独实体来决定谁是primary,好处是server没法自己做否决，只有一个人去决定，但可能会遇到单点故障

问题:脑裂，如果两个服务器之间无法通讯(网络分裂)，他们会互相认为是对方宕机，而将自己作为primary

解决方法1:少数服从多数(服务器要是奇数)

ratf选出一个leader或接受一个日志条目,每次行动都需要半数以上服务器支持，

每个网络分区只能有一个或少量服务器在里面

投票时，指的是所有服务器中的大多数而不是在线服务器中的大多数

如果有2f+1个服务器，那么就可以承受f次故障

如果有三台服务器，大多数就是两台服务器，前后两次term选举的大多数中最少一台服务器是重叠的

两个实现的系统paxos/vsr

有三个完全相同的kv服务器

client实际将请求发送给leader所在的应用层

应用层发给raft层,raft将发送到log中，当完成后,raft将告诉leader

raft与每个副本通讯，直到有半数副本把新操作添加到日志

leader就执行key-value层的修改，返回结果给客户端

当leader意识到一个log state已经被提交了，就需要告诉其他副本这一事件。一般是把这个信息装载到leader发出来的下一个appendentrues请求中(log state)，在其中保存leaderCommit的标识信息

leader状态发生改变(leader更替,日志状态更新)需要给副本一些必要信息，比如新的leadercommit或选举

备份会将操作先放在一边，直到leadercommit再真正执行，

把一个挂掉的备份重新加入raft时，服务器会使用它之前保存在磁盘的日志，

# 选举方法

每个服务器有一个随机计时器，时间到后将自己作为leader候选，并将结果发给其他服务器。

其他服务器如果还没进行投票，就会将票投给发来的那个服务器。

leader候选获得大部分投票时，就会将自己作为leader

三个状态:leader，备份，投票，状态只能是三者之一

当收到选举请求，或leader的heartbeat时，重置计时器

# 任期term

两个或多个副本几乎同时发起选举，就会失败，计时器就会重置为一个随机的数，



如果前一个leader并不知道发生了选举，当他收到一个term比较高的心跳时，会将自己设为备份

如果心跳能发出，但收不到回应，会阻止新的选举，可以的一个选择是:自己下台

当一个选举为leader时，会向所有人发送appendentrues，除了leader，其他所有人都不能被允许发送appendentrues



# 服务器以及他们的日志

如果在某个时刻，s3获取了term6的选举

```
	10 11 12 13
s1  3
s2  3  3  4
s3  3  3  5   6
```

s3发送ae中term为6，同时会把前一个日志中的term信息的(5)也放进去，以及前一个日志的索引prevlogindex 为12放进去，

follow收到AE前，会检查日志条目，在哪1(13下),然后检查条目日志中接收的前一个日志条目和leader发送的信息是否匹配(s2的slot12和s3的slot12的term)是否相同

显然，不匹配，所以s2会拒绝掉这个AE，返回false给leader

s1在solt12处什么都没有，也会拒绝leader

leader在被拒绝后，会为备份维护nextindex字段，leader保存s1,s2的nextindex属性，会将s1和s2的nextindex变为12，并重新发送日志

s3发送prelogindex11及其后面所有的日志条目，s2会删除原本的，而s1无动于衷

接着发送prelogindex10及其之后的日志，s1会接收