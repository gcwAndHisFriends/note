# K百分位数

由(n+1)\*k或1+(n-1)\*k确定位置

线性插值就是a+(b-a)*k

# 四分位数极差

IQR=Q3-Q1

离群点:<Q1-1.5\*IQR或>Q3+1.5\*IQR



# 左右偏态

正偏态，右偏态：均值在众数右边

负偏态，左偏态：均值在众数左边

# 箱形图

如何画

四分位数三条线构成盒体，最大最小值作为上下两端

# 主成分分析

前m个主成分占85%总体方差，即可作为最终主成分

步骤

1:零均值化

将矩阵旋转，求每一行的平均值，然后对原矩阵做减法

2:求样本协方差矩阵
$$
C=\frac{1}{n-1}XX^T
$$
3:求C的特征值和标准正交特征向量

4:将特征向量对应的特征值从上到下按行排列成矩阵

5:将样本投影到新坐标系上

# 如果在没经过预处理的数据集上进行数据挖掘的话，会有哪些问题

使用未经过预处理的数据集进行数据挖掘可能会导致以下问题：

1. 数据质量问题：未经预处理的数据可能包含噪音、错误、缺失值等。这可能会导致分析结果不准确或产生误导性的结论。
2. 数据不一致性：未经过预处理的数据集可能存在不一致的格式、单位、命名规范等问题。这会对分析过程中的数据整合和比较造成困难。
3. 特征选择困难：未经预处理的数据集可能包含大量冗余或不相关的特征。这会导致数据挖掘过程中的特征选择变得困难，影响模型的准确性和解释性。
4. 缺乏数据平衡：未经过预处理的数据集可能存在类别不平衡的问题，其中某些类别的样本数量过少或过多。这会导致模型对于少数类别的预测能力较差。
5. 隐私和安全问题：未经预处理的数据可能包含敏感信息，如个人身份信息、银行账号等。在数据挖掘过程中，必须采取措施保护数据的隐私和安全。

为了解决这些问题，通常需要对数据集进行预处理，包括数据清洗、缺失值处理、特征选择、数据平衡等步骤，以确保挖掘的结果准确、可靠并具有实际应用价值。

# MaxAbsScaler和MinMaxScaler

- MaxAbsScaler：[-1, 1] 将每个特征的数值除以该特征在整个数据集中的绝对值的最大值，将特征的取值范围缩放到[-1, 1]之间
- MinMaxScaler：[0, 1] 将每个特征的数值通过线性变换将其缩放到一个指定的范围，通常是[0, 1]

# 假设12个销售价格记录如下:6,11,205,14,16,216,36,51,12,56,73,93。

## 使用等深划分，将其划分为4箱，16在哪个箱

6, 11, 12, 14, 16, 36, 51, 56, 73, 93, 205, 216。

12除以4等于3，因此每个箱子应该包含3个记录。

按照等深划分的原则，将排序后的记录依次分配到4个箱子中。首先将前3个最小的记录分配到第一个箱子，然后将接下来3个记录分配到第二个箱子，以此类推。

所以，16将在第二个箱子中。

## 使用等宽划分，分为四个箱，16在第几个箱

找出最小值和最大值。在这个例子中，最小值是6，最大值是216。

计算每个箱子的宽度。将最大值和最小值的范围除以箱子的数量。在这个例子中，范围是216-6=210，所以每个箱子的宽度是210/4=52.5。

第一个箱子的边界点是6 + 52.5 = 58.5，第二个箱子的边界点是58.5 + 52.5 = 111，第三个箱子的边界点是111 + 52.5 = 163.5，最后一个箱子的边界点是163.5 + 52.5 = 216。

由于16小于58.5，所以它将被归类到第一个箱子。

## 使用等深分箱法，分成3个箱，用平均值平滑法进行平滑处理，第二个箱的取值是多少

- 第一个箱子：6, 11, 12, 14，平均值为 (6+11+12+14)/4 = 10.75
- 第二个箱子：16, 36, 51, 56，平均值为 (16+36+51+56)/4 = 39.75
- 第三个箱子：73, 93, 205, 216，平均值为 (73+93+205+216)/4 = 146.75

## 使用等宽分箱法，分成3个箱，用平均值平滑法进行平滑处理，第二个箱的取值是多少

第一个箱子将包含6, 11, 12, 14；第二个箱子将包含16, 36, 51, 56；第三个箱子将包含73, 93, 205, 216。

在第二个箱子中，记录的平均值为(16+36+51+56)/4 = 39.75。

# 支持度sup

x={牛奶，鸡蛋，面包}的支持度是这三项同时出现在一条记录中的次数，再除N

推导规则:sub(a->b)=支持度(x{a u b})/N

**置信度**con(a->b)=支持度(a u b)/支持度(a)

# apriori

1:获取每个商品的出现次数，得频繁集项

2:去除次数小于最小支持度的项

3:将商品两两组合，再找频繁集项

4:再加，组成三项的

# 提升度

提升度lift(a,b)=con(a->b)/sup(b)

# 判断是否独立

I(a,b)=sup(a,b)/sup(a)\*sup(b)=0.5<1，是负相关

# 决策树

主要两部分:树生成与树剪枝

分裂后信息熵减少，减少的部分在ID3称信息增益

ID3，C4.5，CART

# 后剪枝

错误率降低剪枝

悲观错误剪枝

代价复杂度剪枝

基于错误的剪枝

# 朴素贝叶斯

朴素原因:朴素贝叶斯假设样本的特征之中彼此独立，没有相关关系。正如我们所知，这个假设在现实生活中是很不真实的。

例：如果有一种水果具有红、椭圆形、直径约3英寸等特征，则该水果可以被判定为是苹果。
尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。

贝叶斯联合概率分布为每个节点给定父节点的条件概率乘积

如果b是a,c父节点

p(a,b,c)=p(a|b)+p(b)+p(c|b)

p(a|b)=p(a,b)/P(b)

# 贝叶斯网络结构

## 同父结构

树形,c是父时，p(a,b,c)=p(c)p(a|c)p(b|c)

p(a,b|c)=p(a,b,c)/p(c)



## 顺序结构

链表结构,a->c->b

p(a,b,c)=p(a)p(c|a)p(b|c)

p(a,b|c)=p(a,b,c)/p(c)

## V形结构

a,b是c父

p(a,b,c)=p(a)p(b)p(c|a,c)

# 组合分类

将多个弱分类器组合成强分类器

在训练集上随机采样，得到新训练集

adaboost:分为k个分类器，第k个分类器尽可能将k-1个分类器没分对的分对。有点高精度，缺点慢

bagging:采T次样，得T个分类器，通过集合策略将弱分类升级为强分类

# 分类器评估方法

真正TP

假正FP

假负FN

真负TN

准确率=(TP+TN)/ALL

精确率=TP/(TP+FP)

召回率=TP/(TP+TN)

F1分数=
$$
\frac{2\times TP}{2\times TP+FP+FN}
$$

# 余弦相似度

$$
\frac{\sum R_1R_2}{\sqrt{\sum R_1^2}\sqrt{\sum R_2^2}}
$$

# K均值 k-means

计算质心，即平均值

对离群点比较敏感

# DBSCAN



优点:处理任意形状，不需指定K，聚类时可发现异常点，对异常点不敏感

缺点:对不均匀数据集效果差，慢，内存开销

# 层次聚类性质

单调性

空间浓缩与扩张
